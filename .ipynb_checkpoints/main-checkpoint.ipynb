{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"sigma.csv\")\n",
    "# null\n",
    "data = data.drop_duplicates()\n",
    "data.duplicated().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ismail Han</td>\n",
       "      <td>ðŸ˜º</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ismail Han</td>\n",
       "      <td>Kaz ta hoilo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ismail Han</td>\n",
       "      <td>Kotha na boijai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ismail Han</td>\n",
       "      <td>Ami vavlam tmi ethan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ismail Han</td>\n",
       "      <td>What's up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106391</th>\n",
       "      <td>Tasin Mahatab</td>\n",
       "      <td>Ata sikis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106392</th>\n",
       "      <td>Tasin Mahatab</td>\n",
       "      <td>V .V.T question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106393</th>\n",
       "      <td>Tasin Mahatab</td>\n",
       "      <td>Most of the important of reproductionðŸ˜†Ismail Han</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106398</th>\n",
       "      <td>Tasin Mahatab</td>\n",
       "      <td>Hu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106399</th>\n",
       "      <td>Ismail Han</td>\n",
       "      <td>Eto grp lage?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70513 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                label                                              text\n",
       "0          Ismail Han                                                 ðŸ˜º\n",
       "1          Ismail Han                                      Kaz ta hoilo\n",
       "2          Ismail Han                                   Kotha na boijai\n",
       "3          Ismail Han                              Ami vavlam tmi ethan\n",
       "4          Ismail Han                                        What's up?\n",
       "...               ...                                               ...\n",
       "106391  Tasin Mahatab                                         Ata sikis\n",
       "106392  Tasin Mahatab                                   V .V.T question\n",
       "106393  Tasin Mahatab  Most of the important of reproductionðŸ˜†Ismail Han\n",
       "106398  Tasin Mahatab                                                Hu\n",
       "106399     Ismail Han                                     Eto grp lage?\n",
       "\n",
       "[70513 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st e word tokenize\n",
    "##### puncuation remove\n",
    "#### remove special char\n",
    "#### remove stop word\n",
    "#### Lemmatization\n",
    "#### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       [ðŸ˜º]\n",
       "1                                          [Kaz, ta, hoilo]\n",
       "2                                       [Kotha, na, boijai]\n",
       "3                                 [Ami, vavlam, tmi, ethan]\n",
       "4                                               [Whats, up]\n",
       "                                ...                        \n",
       "106391                                         [Ata, sikis]\n",
       "106392                                    [V, VT, question]\n",
       "106393    [Most, of, the, important, of, reproductionðŸ˜†Is...\n",
       "106398                                                 [Hu]\n",
       "106399                                     [Eto, grp, lage]\n",
       "Name: text, Length: 70513, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "#import nltk\n",
    "import banglanltk as nltk\n",
    "from banglanltk import word_tokenize\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkkt_tab')\n",
    "\n",
    "data['text'] = data['text'].fillna('') # nan(not a number) also kind of NONE type thing. ekhane amra oi value gula change kore dibo\n",
    "data['text'].apply(lambda x: word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        ðŸ˜º\n",
       "1                                             Kaz ta hoilo\n",
       "2                                          Kotha na boijai\n",
       "3                                     Ami vavlam tmi ethan\n",
       "4                                                 Whats up\n",
       "                                ...                       \n",
       "106391                                           Ata sikis\n",
       "106392                                       V VT question\n",
       "106393    Most of the important of reproductionðŸ˜†Ismail Han\n",
       "106398                                                  Hu\n",
       "106399                                        Eto grp lage\n",
       "Name: text, Length: 70513, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punc\n",
    "import string\n",
    "def remove_punc(text):\n",
    "  trans = str.maketrans('', '', string.punctuation)\n",
    "  return text.translate(trans)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_punc)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                         \n",
       "1                                             Kaz ta hoilo\n",
       "2                                          Kotha na boijai\n",
       "3                                     Ami vavlam tmi ethan\n",
       "4                                                 Whats up\n",
       "                                ...                       \n",
       "106391                                           Ata sikis\n",
       "106392                                       V VT question\n",
       "106393    Most of the important of reproduction Ismail Han\n",
       "106398                                                  Hu\n",
       "106399                                        Eto grp lage\n",
       "Name: text, Length: 70513, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### remove special char (abcd er baire habijabi)\n",
    "import re\n",
    "def remove_noise(text):\n",
    "  t = re.sub('[^a-zA-Z]', ' ', text)\n",
    "  return t\n",
    "data['text'] = data['text'].apply(remove_noise)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                          \n",
       "1                              kaz ta hoilo\n",
       "2                           kotha na boijai\n",
       "3                      ami vavlam tmi ethan\n",
       "4                                     whats\n",
       "                        ...                \n",
       "106391                            ata sikis\n",
       "106392                        v vt question\n",
       "106393    important reproduction ismail han\n",
       "106398                                   hu\n",
       "106399                         eto grp lage\n",
       "Name: text, Length: 70513, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### remove stop word\n",
    "import nltk\n",
    "from bn_nlp.preprocessing import ban_processing\n",
    "nltk.download('stopwords')\n",
    "from bn_nlp.preprocessing import ban_processing\n",
    "bp=ban_processing()\n",
    "from nltk.corpus import stopwords\n",
    "# bangla\n",
    "with open('bn_nlp/dataset/stop_word.txt','r') as sto:\n",
    "    #stops=sto.read()\n",
    "    stops=set(sto.read().splitlines())\n",
    "    \n",
    "sw = set(stopwords.words('english'))\n",
    "combined_stopwords = sw.union(stops)\n",
    "def remove_sws(text):\n",
    "    # low char\n",
    "    try:\n",
    "        text = text.lower()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    non_stop_word = []\n",
    "    split_text = text.split()\n",
    "    for new in split_text:\n",
    "        if new not in combined_stopwords:\n",
    "            non_stop_word.append(new)\n",
    "            # from nltk.stem import WordNetLemmatizer\n",
    "            # lemmatizer = WordNetLemmatizer()\n",
    "            # new = lemmatizer.lemmatize(new)\n",
    "            # non_stop_word.append(new)\n",
    "    return \" \".join(non_stop_word)\n",
    "            \n",
    "# def remove_sws(text):\n",
    "#   s = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "#   return \" \".join(s)\n",
    "data['text'] = data['text'].apply(remove_sws)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization\n",
    "# similar word\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(text):\n",
    "    \n",
    "    lemmatize_Word = []\n",
    "    split_text = text.split()\n",
    "    for new in split_text:\n",
    "        if new not in sw:\n",
    "            lemmatize_Word.append(new)\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            new = lemmatizer.lemmatize(new)\n",
    "            lemmatize_Word.append(new)\n",
    "    return \" \".join(lemmatize_Word)\n",
    "           \n",
    "def lemma(text):\n",
    "  l = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "  return \" \".join(l)\n",
    "\n",
    "data['text'] = data['text'].apply(lemma)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoder\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder =LabelEncoder()\n",
    "\n",
    "# data['v1']=encoder.fit_transform(data['v1'])\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(max_features=3000)\n",
    "x = tf.fit_transform(data['text']).toarray()\n",
    "y = data['label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(x)\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "# Create the instance of Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "# Fit the data\n",
    "clf.fit(X_train, y_train)\n",
    "# Making prediction\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# your_sen = [\"join the meeting\"]\n",
    "# your_sen = tf.transform(your_sen).toarray()\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(y_pred)\n",
    "# print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# tokeize korte hobe\n",
    "your_sen = [\"join and love\"]\n",
    "\n",
    "# Preprocess the new sentence before vectorizing\n",
    "your_sen = [preprocess_text(sen) for sen in your_sen]\n",
    "\n",
    "# Transform the new sentence using the same vectorizer\n",
    "your_sen_transformed = tf.transform(your_sen).toarray()\n",
    "#data['v2'].apply(lambda x: word_tokenize(x))\n",
    "# Predict the label for the new sentence\n",
    "your_sen_pred = clf.predict(your_sen_transformed)\n",
    "\n",
    "# Print the prediction for the new sentence\n",
    "print(\"Prediction for your sentence:\", 'Spam' if your_sen_pred[0] == 1 else 'Ham')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
